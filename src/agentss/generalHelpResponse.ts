// src\utils\responses\greetingResponse.ts
import { ChatOpenAI } from "@langchain/openai";
import { ConversationChain } from "langchain/chains";
import { BufferMemory } from "langchain/memory";
import { PromptTemplate } from "@langchain/core/prompts";
import {
  getLangChainMemory,
  saveLangChainMemory,
} from "./helpers/langchainMemoryStore";
import { chatModel } from "@/config/model";

const prompt = PromptTemplate.fromTemplate(`
You're a friendly AI assistant for a bookstore. When a user sends a greeting, reply warmly and briefly offer help.

User: {input}
Assistant:
`);

export const generateGreetingResponse = async (
  input: string,
  sessionId: string
): Promise<string> => {
  try {
    const memory = new BufferMemory({
      returnMessages: true,
      memoryKey: "chat_history",
      inputKey: "input",
    });

    // ğŸ§  Load previous memory
    const previousMessages = getLangChainMemory(sessionId);
    if (previousMessages) {
      await memory.chatHistory.addMessages(previousMessages);
    }

    const chain = new ConversationChain({
      llm: chatModel,
      memory,
      prompt,
    });

    const result = await chain.call({ input });

    const updatedMessages = await memory.chatHistory.getMessages();
    console.log("ğŸ§  BufferMemory (LangChain) messages:", updatedMessages);

    // ğŸ’¾ Save updated memory
    saveLangChainMemory(sessionId, updatedMessages);

    return typeof result.response === "string"
      ? result.response.trim()
      : "ğŸ‘‹ Hi! How can I assist you today?";
  } catch (err) {
    console.error("âŒ [generateGreetingResponse] Error:", err);
    return "ğŸ‘‹ Hello! Let me know how I can help.";
  }
};
